{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', \n",
    "                    datefmt='%d-%b-%y %H:%M:%S')\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def generate_data(path, file_system='local', **args):\n",
    "    \"\"\"\n",
    "        n_samples=100,n_features=4,\n",
    "        class_sep=1.0, n_informative=2,\n",
    "        n_redundant=2, random_state=rs\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = make_classification(**args)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=['x_'+str(i+1) for i in range(X.shape[1])])\n",
    "    df = pd.concat([df, pd.DataFrame({'y':y})], axis=1)\n",
    "    \n",
    "    if file_system =='local':\n",
    "        df.to_csv(path, index=False)\n",
    "        print(df.head())\n",
    "        logging.info(f'Dataset was generated successfully and saved in {path} ')\n",
    "    \n",
    "    elif file_system =='hdfs':\n",
    "        cluster_manager = 'yarn'\n",
    "        spark = SparkSession.builder\\\n",
    "        .master(cluster_manager)\\\n",
    "        .appName(\"myapp\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        spark_df.show(5)\n",
    "        spark_df.limit(10000).write.mode('overwrite').parquet(path)\n",
    "        logging.info(f'Dataset was generated successfully and saved in hdfs://{path} ')\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17-Jun-19 11:51:30 - Dataset was generated successfully and saved in /tmp/original_data.csv \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        x_1       x_2       x_3       x_4  y\n",
      "0  2.257082  3.174625  2.783862 -0.478191  0\n",
      "1  1.350390  1.949810  1.760475 -0.403942  1\n",
      "2  1.159745  1.119558  0.468043  0.949132  1\n",
      "3 -1.360882 -1.425550 -0.759553 -0.852599  0\n",
      "4  0.033744 -0.675969 -1.319113  1.682271  1\n"
     ]
    }
   ],
   "source": [
    "generate_data(path='/tmp/original_data.csv', file_system='local',\n",
    "                           n_samples=100,n_features=4,\n",
    "                            class_sep=1.0, n_informative=2,\n",
    "                            n_redundant=2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_gathering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.tracking.client.MlflowClient at 0x7fb7a0612278>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr = client.list_experiments()\n",
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/project/mlruns_modified/0/0d14d05b5c604619abd96b08e7c61328/artifacts'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.tracking.client.MlflowClient at 0x7ff9ba675f28>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/root/project/mlruns_modified/0', experiment_id='0', lifecycle_stage='active', name='exp1'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_experiment_by_name('exp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "exp_name = exp_workflow | exp_id = 0\n",
      "artifact_location = hdfs:///tmp/exp_workflow\n",
      "        x_1       x_2       x_3       x_4  y\n",
      "0  2.257082  3.174625  2.783862 -0.478191  0\n",
      "1  1.350390  1.949810  1.760475 -0.403942  1\n",
      "2  1.159745  1.119558  0.468043  0.949132  1\n",
      "3 -1.360882 -1.425550 -0.759553 -0.852599  0\n",
      "4  0.033744 -0.675969 -1.319113  1.682271  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import zipfile\n",
    "import pyspark\n",
    "import mlflow\n",
    "import click\n",
    "import subprocess\n",
    "import glob\n",
    "import sklearn\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SQLContext, Row, SparkSession\n",
    "\n",
    "print(mlflow.__version__) # it must be 1.0\n",
    "\n",
    "uri = '/root/project/mlruns_modified'\n",
    "exp_name = 'exp_workflow'\n",
    "mlflow.set_tracking_uri(uri)\n",
    "\n",
    "artifact_location = os.path.join('hdfs:///tmp', exp_name)\n",
    "exp_id = mlflow.create_experiment(exp_name, \n",
    "                                  artifact_location=artifact_location)\n",
    "# exp_id = mlflow.create_experiment('exp_workflow')\n",
    "# exp_id = 0\n",
    "print(f'exp_name = {exp_name} | exp_id = {exp_id}')\n",
    "print(f'artifact_location = {artifact_location}')\n",
    "# print(exp_id)\n",
    "# mlflow.set_experiment('exp_workflow')\n",
    "# mlflow.server.\n",
    "# @click.command(help=\"Script that gather a dataset from a source data / repository\")\n",
    "# @click.option(\"--path\", help=\"Dataset's path readable by Spark. Located in HDFS.\")\n",
    "def gather_data(source):\n",
    "#     with mlflow.start_run(experiment_id = exp_id) as mlrun:\n",
    "    with mlflow.start_run() as mlrun:\n",
    "#         path = f'hdfs://{path}'\n",
    "        cluster_manager = 'yarn'\n",
    "        spark = SparkSession.builder\\\n",
    "        .master(cluster_manager)\\\n",
    "        .appName(\"gather_data\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "        df = pd.read_csv(source)\n",
    "        print(df.head())\n",
    "        logging.info(f'Dataset: {source} was read successfully ') \n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        parquet_file = os.path.join(artifact_location, 'original_data.parquet')\n",
    "        spark_df.limit(10000).write.mode('overwrite').parquet(parquet_file)\n",
    "        logging.info(f'Dataset was saved in {parquet_file} ')\n",
    "        \n",
    "\n",
    "#         mlflow.log_artifact(path, path_artifact)\n",
    "        mlflow.log_artifact(source)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "gather_data(source='/tmp/original_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+---+\n",
      "|                 x_1|                x_2|                x_3|                 x_4|  y|\n",
      "+--------------------+-------------------+-------------------+--------------------+---+\n",
      "|   2.257081698491103| 3.1746248283618392|   2.78386208858466| -0.4781910157533783|  0|\n",
      "|  1.3503902613309968| 1.9498100498575857| 1.7604752337080989|-0.40394190691776144|  1|\n",
      "|   1.159745242939114| 1.1195576811232828|  0.468043236945129|  0.9491322319035848|  1|\n",
      "| -1.3608824689920729| -1.425549824985629|-0.7595532711067904| -0.8525989473220963|  0|\n",
      "|0.033743594128649315|-0.6759687711021607|-1.3191129184946184|  1.6822707033005866|  1|\n",
      "+--------------------+-------------------+-------------------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pyspark\n",
    "import mlflow\n",
    "import click\n",
    "import sklearn\n",
    "import logging\n",
    "import tempfile\n",
    "\n",
    "# exp_name = 'exp_workflow'\n",
    "# artifact_location = os.path.join('hdfs:///tmp', exp_name)\n",
    "exp_name = 'exp_workflow'\n",
    "artifact_location = os.path.join('hdfs:///tmp', exp_name)\n",
    "\n",
    "def preprocessing(source, target):\n",
    "#     with mlflow.start_run(experiment_id = exp_id) as mlrun:\n",
    "    with mlflow.start_run(nested=True) as mlrun:\n",
    "#         path = f'hdfs://{path}'\n",
    "        cluster_manager = 'yarn'\n",
    "        spark = SparkSession.builder\\\n",
    "        .master(cluster_manager)\\\n",
    "        .appName(\"gather_data\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "        spark_df = spark.read.parquet(source)\n",
    "        logging.info(f'Dataset: hdfs://{source} was read successfully ') \n",
    "        spark_df.show(5)\n",
    "        \n",
    "        #Some preprocessing steps here\n",
    "        spark_df = spark_df.dropna()\n",
    "        spark_df.limit(10000).write.mode('overwrite').parquet(target)\n",
    "        logging.info(f'Cleaned dataset was saved in {target} ')\n",
    "        \n",
    "        df_cleaned = spark_df.toPandas()\n",
    "        mlflow.log_param(key='n_samples', value=len(df_cleaned))\n",
    "        mlflow.log_param(key='n_features', value=len(df_cleaned.columns)-1)\n",
    "        dict_types = dict([(x,str(y)) for x,y in zip(df.columns, df.dtypes.values)])\n",
    "        mlflow.log_param(key='dtypes', value=dict_types)\n",
    "        mlflow.log_param(key='classes', value=df_cleaned['y'].unique())\n",
    "        mlflow.log_param(key='problem_type', value='classification')\n",
    "        \n",
    "\n",
    "        \n",
    "        tmpdir = tempfile.mkdtemp()\n",
    "        tmp_file = os.path.join(tmpdir, 'cleaned_data.csv')\n",
    "        df_cleaned.to_csv(tmp_file, index=False)\n",
    "#         path_cleaned_data = '/tmp/exp_workflow/original_cleaned.parquet'\n",
    "        mlflow.log_artifact(tmp_file)\n",
    "        \n",
    "#         mlflow.log_artifact(path, path_artifact)\n",
    "#         mlflow.log_artifact('/root/project/ui_run', path_artifact)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "preprocessing('/tmp/exp_workflow/original_data.parquet', '/tmp/exp_workflow/cleaned_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature_engineering.py\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+---+\n",
      "|                 x_1|                x_2|                x_3|                 x_4|  y|\n",
      "+--------------------+-------------------+-------------------+--------------------+---+\n",
      "|   2.257081698491103| 3.1746248283618392|   2.78386208858466| -0.4781910157533783|  0|\n",
      "|  1.3503902613309968| 1.9498100498575857| 1.7604752337080989|-0.40394190691776144|  1|\n",
      "|   1.159745242939114| 1.1195576811232828|  0.468043236945129|  0.9491322319035848|  1|\n",
      "| -1.3608824689920729| -1.425549824985629|-0.7595532711067904| -0.8525989473220963|  0|\n",
      "|0.033743594128649315|-0.6759687711021607|-1.3191129184946184|  1.6822707033005866|  1|\n",
      "+--------------------+-------------------+-------------------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Train on 64 samples, validate on 16 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6933 - acc: 0.4219 - val_loss: 0.6884 - val_acc: 0.6250\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6923 - acc: 0.4219 - val_loss: 0.6874 - val_acc: 0.6875\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6912 - acc: 0.4531 - val_loss: 0.6864 - val_acc: 0.6875\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6901 - acc: 0.5000 - val_loss: 0.6853 - val_acc: 0.7500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6889 - acc: 0.5312 - val_loss: 0.6841 - val_acc: 0.7500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6876 - acc: 0.6406 - val_loss: 0.6830 - val_acc: 0.7500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6864 - acc: 0.7656 - val_loss: 0.6818 - val_acc: 0.8125\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6851 - acc: 0.8281 - val_loss: 0.6806 - val_acc: 0.8750\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6838 - acc: 0.8750 - val_loss: 0.6794 - val_acc: 0.8750\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6825 - acc: 0.8750 - val_loss: 0.6782 - val_acc: 0.8750\n",
      "[0.6805938601493835, 0.8875]\n",
      "[0.6793184876441956, 0.8500000238418579]\n",
      "The model had a ACC on the test set of [0.6793184876441956, 0.8500000238418579]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pyspark\n",
    "import mlflow\n",
    "import click\n",
    "import sklearn\n",
    "import logging\n",
    "import tempfile\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "# exp_name = 'exp_workflow'\n",
    "# artifact_location = os.path.join('hdfs:///tmp', exp_name)\n",
    "\n",
    "def modeling(source, target):\n",
    "#     with mlflow.start_run(experiment_id = exp_id) as mlrun:\n",
    "    with mlflow.start_run(nested=True) as mlrun:\n",
    "#         path = f'hdfs://{path}'\n",
    "        cluster_manager = 'yarn'\n",
    "        spark = SparkSession.builder\\\n",
    "        .master(cluster_manager)\\\n",
    "        .appName(\"gather_data\")\\\n",
    "        .config(\"spark.driver.allowMultipleContexts\", \"true\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "        spark_df = spark.read.parquet(source)\n",
    "        logging.info(f'Dataset: hdfs://{source} was read successfully ') \n",
    "        spark_df.show(5)\n",
    "        \n",
    "        df = spark_df.toPandas()\n",
    "        X = df.loc[:, df.columns != 'y']\n",
    "        y = df['y']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(30, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#         filepath = '/tmp/ALS_checkpoint_weights.hdf5'\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, mode='auto')\n",
    "\n",
    "        model.fit(X_train, y_train, validation_split=.2, verbose=2, epochs=10,\n",
    "                  batch_size=128, shuffle=False)\n",
    "\n",
    "        train_acc = model.evaluate(X_train, y_train, verbose=2)\n",
    "        print(train_acc)\n",
    "        test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "        print(test_acc)\n",
    "        mlflow.log_metric(\"train_acc\", round(train_acc[1], 2))\n",
    "        mlflow.log_metric(\"test_acc\", round(test_acc[1], 2))\n",
    "\n",
    "        print('The model had a ACC on the test set of {0}'.format(test_acc))\n",
    "        # TODO: Check the right path of  the keras model (artifact)\n",
    "#         mlflow.keras.log_model(model, \"models\")\n",
    "        #     mlflow.keras.save_model(model, \"keras-model\")\n",
    "        \n",
    "#         mlflow.log_artifact(path, path_artifact)\n",
    "#         mlflow.log_artifact('/root/project/ui_run', path_artifact)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "modeling('/tmp/exp_workflow/cleaned_data.parquet', '/tmp/exp_workflow/final_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run matched, but is not FINISHED, so skipping (run_id=241ab9690acf49d9b14488d632aa2bbf, status=FAILED)\n",
      "No matching run has been found.\n",
      "2019/06/18 08:48:23 INFO mlflow.projects: === Created directory /tmp/tmp8y_9ekmn for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/06/18 08:48:23 INFO mlflow.projects: === Running command 'python data_gathering.py --source /tmp/original_data.csv' in run with ID '3449c6349d124db1ba87f30af6a3c20c' === \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching new run for entrypoint=data_gathering and parameters={'source': '/tmp/original_data.csv'}\n"
     ]
    },
    {
     "ename": "ExecutionException",
     "evalue": "Run (ID '3449c6349d124db1ba87f30af6a3c20c') failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExecutionException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e08e81bf413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m workflow(source='/tmp/original_data.csv',\n\u001b[0;32m--> 109\u001b[0;31m         keras_hidden_units=20)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6e08e81bf413>\u001b[0m in \u001b[0;36mworkflow\u001b[0;34m(source, keras_hidden_units)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mgit_commit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlflow_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLFLOW_GIT_COMMIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgit_commit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mdata_gathering_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_or_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_gathering\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgit_commit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gathering_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m#         ratings_csv_uri = os.path.join(gather_data_run.info.artifact_uri, \"ratings-csv-dir\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6e08e81bf413>\u001b[0m in \u001b[0;36m_get_or_run\u001b[0;34m(entrypoint, parameters, git_commit, use_cache)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexisting_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Launching new run for entrypoint=%s and parameters=%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentrypoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msubmitted_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentrypoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_conda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmitted_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/mlflow/projects/__init__.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(uri, entry_point, version, parameters, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\u001b[0m\n\u001b[1;32m    230\u001b[0m         use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msynchronous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0m_wait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmitted_run_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msubmitted_run_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/mlflow/projects/__init__.py\u001b[0m in \u001b[0;36m_wait_for\u001b[0;34m(submitted_run_obj)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0m_maybe_set_run_terminated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FAILED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run (ID '%s') failed\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== Run (ID '%s') interrupted, cancelling run ===\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutionException\u001b[0m: Run (ID '3449c6349d124db1ba87f30af6a3c20c') failed"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downloads the MovieLens dataset, ETLs it into Parquet, trains an\n",
    "ALS model, and uses the ALS model to train a Keras neural network.\n",
    "\n",
    "See README.rst for more details.\n",
    "\"\"\"\n",
    "\n",
    "import click\n",
    "import os\n",
    "\n",
    "\n",
    "import mlflow\n",
    "from mlflow.utils import mlflow_tags\n",
    "from mlflow.entities import RunStatus\n",
    "from mlflow.utils.logging_utils import eprint\n",
    "import six\n",
    "\n",
    "from mlflow.tracking.fluent import _get_experiment_id\n",
    "\n",
    "print(mlflow.__version__) # it must be 1.0\n",
    "\n",
    "uri = '/root/project/mlruns_modified'\n",
    "mlflow.set_tracking_uri(uri)\n",
    "\n",
    "exp_name = 'exp_workflow'\n",
    "artifact_location = os.path.join('hdfs:///tmp', exp_name)\n",
    "exp_id = mlflow.create_experiment(exp_name, \n",
    "                                  artifact_location=artifact_location)\n",
    "\n",
    "print(f'exp_name = {exp_name} | exp_id = {exp_id}')\n",
    "print(f'artifact_location = {artifact_location}')\n",
    "\n",
    "\n",
    "def _already_ran(entry_point_name, parameters, git_commit, experiment_id=None):\n",
    "    \"\"\"Best-effort detection of if a run with the given entrypoint name,\n",
    "    parameters, and experiment id already ran. The run must have completed\n",
    "    successfully and have at least the parameters provided.\n",
    "    \"\"\"\n",
    "    experiment_id = experiment_id if experiment_id is not None else _get_experiment_id()\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    all_run_infos = reversed(client.list_run_infos(experiment_id))\n",
    "    for run_info in all_run_infos:\n",
    "        full_run = client.get_run(run_info.run_id)\n",
    "        tags = full_run.data.tags\n",
    "        if tags.get(mlflow_tags.MLFLOW_PROJECT_ENTRY_POINT, None) != entry_point_name:\n",
    "            continue\n",
    "        match_failed = False\n",
    "        for param_key, param_value in six.iteritems(parameters):\n",
    "            run_value = full_run.data.params.get(param_key)\n",
    "            if run_value != param_value:\n",
    "                match_failed = True\n",
    "                break\n",
    "        if match_failed:\n",
    "            continue\n",
    "\n",
    "        if run_info.status != RunStatus.FINISHED:\n",
    "            eprint((\"Run matched, but is not FINISHED, so skipping \"\n",
    "                    \"(run_id=%s, status=%s)\") % (run_info.run_id, run_info.status))\n",
    "            continue\n",
    "\n",
    "        previous_version = tags.get(mlflow_tags.MLFLOW_GIT_COMMIT, None)\n",
    "        if git_commit != previous_version:\n",
    "            eprint((\"Run matched, but has a different source version, so skipping \"\n",
    "                    \"(found=%s, expected=%s)\") % previous_version, git_commit)\n",
    "            continue\n",
    "        return client.get_run(run_info.run_id)\n",
    "    eprint(\"No matching run has been found.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# TODO(aaron): This is not great because it doesn't account for:\n",
    "# - changes in code\n",
    "# - changes in dependant steps\n",
    "def _get_or_run(entrypoint, parameters, git_commit, use_cache=True):\n",
    "    existing_run = _already_ran(entrypoint, parameters, git_commit)\n",
    "    if use_cache and existing_run:\n",
    "        print(\"Found existing run for entrypoint=%s and parameters=%s\" % (entrypoint, parameters))\n",
    "        return existing_run\n",
    "    print(\"Launching new run for entrypoint=%s and parameters=%s\" % (entrypoint, parameters))\n",
    "    submitted_run = mlflow.run(\".\", entrypoint, parameters=parameters, use_conda=False)\n",
    "    return mlflow.tracking.MlflowClient().get_run(submitted_run.run_id)\n",
    "\n",
    "\n",
    "# @click.command()\n",
    "# @click.option(\"--source\", help=\"Dataset's path in local.\")\n",
    "# @click.option(\"--keras-hidden-units\", default=20, type=int)\n",
    "def workflow(source, keras_hidden_units):\n",
    "    # Note: The entrypoint names are defined in MLproject. The artifact directories\n",
    "    # are documented by each step's .py file.\n",
    "    with mlflow.start_run() as active_run:\n",
    "#         os.environ['SPARK_CONF_DIR'] = os.path.abspath('.')\n",
    "        git_commit = active_run.data.tags.get(mlflow_tags.MLFLOW_GIT_COMMIT)\n",
    "        print(git_commit)\n",
    "        data_gathering_run = _get_or_run(\"data_gathering\", {'source': source}, git_commit)\n",
    "        print(data_gathering_run)\n",
    "#         ratings_csv_uri = os.path.join(gather_data_run.info.artifact_uri, \"ratings-csv-dir\")\n",
    "#         etl_data_run = _get_or_run(\"etl_data\",\n",
    "#                                    {\"ratings_csv\": ratings_csv_uri,\n",
    "#                                     \"max_row_limit\": max_row_limit},\n",
    "#                                    git_commit)\n",
    "#         ratings_parquet_uri = os.path.join(etl_data_run.info.artifact_uri, \"ratings-parquet-dir\")\n",
    "\n",
    "#         # We specify a spark-defaults.conf to override the default driver memory. ALS requires\n",
    "#         # significant memory. The driver memory property cannot be set by the application itself.\n",
    "#         als_run = _get_or_run(\"als\", \n",
    "#                               {\"ratings_data\": ratings_parquet_uri, \"max_iter\": str(als_max_iter)},\n",
    "#                               git_commit)\n",
    "#         als_model_uri = os.path.join(als_run.info.artifact_uri, \"als-model\")\n",
    "\n",
    "#         keras_params = {\n",
    "#             \"ratings_data\": ratings_parquet_uri,\n",
    "#             \"als_model_uri\": als_model_uri,\n",
    "#             \"hidden_units\": keras_hidden_units,\n",
    "#         }\n",
    "#         _get_or_run(\"train_keras\", keras_params, git_commit, use_cache=False)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     workflow()\n",
    "\n",
    "workflow(source='/tmp/original_data.csv',\n",
    "        keras_hidden_units=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run matched, but is not FINISHED, so skipping (run_id=7aa72ee90c124734bb393e20b8f666cc, status=FAILED)\n",
      "No matching run has been found.\n",
      "2019/06/18 08:35:54 INFO mlflow.projects: === Created directory /tmp/tmpto_zwkg4 for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2019/06/18 08:35:54 INFO mlflow.projects: === Running command 'python gather_data.py --source /tmp/original_data.csv' in run with ID '4033914092f74eeaa1d01283bb1d1de3' === \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching new run for entrypoint=gather_data and parameters={'source': '/tmp/original_data.csv'}\n"
     ]
    },
    {
     "ename": "ExecutionException",
     "evalue": "Run (ID '4033914092f74eeaa1d01283bb1d1de3') failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExecutionException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-03763562b18d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgit_commit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlflow_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLFLOW_GIT_COMMIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgit_commit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgather_data_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_or_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gather_data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgit_commit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mgather_data_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-9169fa20c0ee>\u001b[0m in \u001b[0;36m_get_or_run\u001b[0;34m(entrypoint, parameters, git_commit, use_cache)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexisting_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Launching new run for entrypoint=%s and parameters=%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentrypoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msubmitted_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentrypoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_conda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmitted_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/mlflow/projects/__init__.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(uri, entry_point, version, parameters, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\u001b[0m\n\u001b[1;32m    230\u001b[0m         use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msynchronous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0m_wait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmitted_run_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msubmitted_run_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/mlflow/projects/__init__.py\u001b[0m in \u001b[0;36m_wait_for\u001b[0;34m(submitted_run_obj)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0m_maybe_set_run_terminated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FAILED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run (ID '%s') failed\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== Run (ID '%s') interrupted, cancelling run ===\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutionException\u001b[0m: Run (ID '4033914092f74eeaa1d01283bb1d1de3') failed"
     ]
    }
   ],
   "source": [
    "source='/tmp/original_data.csv'\n",
    "with mlflow.start_run() as active_run:\n",
    "    git_commit = active_run.data.tags.get(mlflow_tags.MLFLOW_GIT_COMMIT)\n",
    "    print(git_commit)\n",
    "    gather_data_run = _get_or_run(\"gather_data\", {'source': source}, git_commit)\n",
    "    gather_data_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
